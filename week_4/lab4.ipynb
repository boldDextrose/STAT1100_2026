{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAT1100 Data Communication and Modelling: Week 4\n",
    "\n",
    "This week we will be looking at reading, saving, and analyzing data from files. To complete this\n",
    "lab you will need to download the following files and place them in the same folder you are\n",
    "running python/jupyter from:\n",
    "\n",
    "- 04cars.txt\n",
    "- 04cars.xlsx\n",
    "- brainsize.csv\n",
    "- example.stat1100\n",
    "- heart.json\n",
    "- iris.xlsx\n",
    "- mystery_file\n",
    "- titanic.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reading Data From Files\n",
    "\n",
    "The first step to reading some data from a file is knowing what type of file it is. Often\n",
    "we can tell by the file extension, that is the part of the file name after the dot, e.g. the docx part\n",
    "of `notes.docx` shows you that it is a word document. However, sometimes we can't tell either\n",
    "due the having the wrong extension or none at all. This is not a big problem in python though,\n",
    "as we can use the [python-magic](https://pypi.org/project/python-magic/) library to help us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -U python-magic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this package in python is reasonably simple, just import it and then use the `from_file` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import magic\n",
    "\n",
    "magic.from_file(\"mystery_file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we know the filetype, we know what strategy we need to read and translate it into a python-based\n",
    "object/collection. In the following, we will provide techniques to read common file types that store data.\n",
    "For many of these there are often techniques specific to the file type and a pandas/numpy based approach,\n",
    "either usually works fine while the former allows for more flexibility and the latter is much easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Files (TXT)\n",
    "\n",
    "Text files are usually one of the most basic file types, and within a structured file context tend to only\n",
    "store basic descriptions to accompany other data that holds structure. With text files, we can see the basics\n",
    "in python file reading. In the following example code, we will read the text file '04cars.txt' and store in\n",
    "a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "description = \"\"\n",
    "with open('04cars.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        description += line\n",
    "\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first line we open a `with` block over the file opening 'context', this ensures that our file is closed once\n",
    "the block ends (when the new indent from the colon ends) preventing potential file corruption. This could alternatively\n",
    "be done without the `with` block, instead the file would be opened with a `f = open('notes.txt', 'r')` statement and\n",
    "closed with `f.close()`. You may notice that in the `open` statement we first specify the file name and then the 'mode',\n",
    "this mode just states our use for the file, in this case we are reading the file with the `r` mode.\n",
    "\n",
    "In the next two lines, we create a loop iterating over each line of the file, and add each line to our `description` string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comma Separated Values (CSV)\n",
    "\n",
    "The csv file is a simple file type that stores values in a tabulated format by separating columns with commas (sometimes\n",
    "also a different character like a space or colon), and rows by new lines. For example,\n",
    "\n",
    "```csv\n",
    "id,name,age\n",
    "1,John,20\n",
    "2,Jane,21\n",
    "```\n",
    "\n",
    "Python has a built-in library for reading csv files, which is called `csv`. We can use this library to read 'brainsize.csv'\n",
    "as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "data = []\n",
    "with open('brainsize.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        data.append(row)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example uses the `csv.reader` function to interpret the file as a csv file instead of text, this transforms the\n",
    "rows of the file into lists of the values in each row. This next allows us to easily construct the 2D list `data` which\n",
    "contains the data from the file, where rows are lists of the individual values.\n",
    "\n",
    "Alternatively, both numpy and pandas provide functions to read csv files, they place the data into a numpy array or\n",
    "pandas dataframe respectively. We show both in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First we import both libraries, you would only need one or the other\n",
    "# depending on what collection you want the data in\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# numpy csv reading\n",
    "data = np.genfromtxt('brainsize.csv', delimiter=',')\n",
    "print(data)  \n",
    "# Notice that it places nan (not a number) for each of the string values\n",
    "\n",
    "# pandas csv reading\n",
    "data = pd.read_csv('brainsize.csv')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JavaScript Object Notation (JSON)\n",
    "\n",
    "[JSON](json.org) files are also a simple file type that stores data in a structured format, yet they are more flexible than csv.\n",
    "JSON files originate as the format by which JavaScript objects are represented, and thus have similar flexibility to collections\n",
    "seen in programming languages. The following is an example of what JSON file looks like:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"id\": 1,\n",
    "    \"name\": \"John\",\n",
    "    \"age\": 20,\n",
    "    \"address\": {\n",
    "        \"street\": \"Main Street\",\n",
    "        \"city\": \"London\",\n",
    "        \"postcode\": \"E1 2AB\"\n",
    "    },\n",
    "    \"phone\": [\n",
    "        \"0123456789\",\n",
    "        \"9876543210\"\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Fortunately for python, the structure of JSON files is equivalent to the structure of a python dictionary (+ list). So reading, analyzing and\n",
    "writing remains relatively simple. The json library built into python loads the files directly into dictionaries, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"heart.json\", 'rb') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, Pandas also provides a function to read json files, which is called `read_json`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_json('heart.json')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pandas function again provides a dataframe representing the data in the json file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excel Spreadsheets (XLS/XLSX)\n",
    "\n",
    "Excel spreadsheets are a much more complex file type which stores data in the tabulated spreadsheet format. As you\n",
    "may know, these files are created using the Microsoft Excel or libreoffice (etc.) applications. Even inside of those\n",
    "applications we can perform substantial analysis of the data. But, python also provide libraries to interact with\n",
    "excel files, providing us with more flexibility in both analysis and in saving or converting data collections.\n",
    "\n",
    "Python does not have any built-in libraries for reading excel files, but we can use the combination of the openpyxl\n",
    "and pandas libraries to read excel files. First make sure both are installed by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -U openpyxl pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can read the excel file '04cars.xlsx' as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_excel('04cars.xlsx')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, the excel tables are nearly equivalent to the pandas dataframe there is almost no loss of information here, though\n",
    "it appears we are missing the ability to see different sheets in the file. But, it is actually possible to read the other sheets,\n",
    "it just requires a new dataframe for each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_excel('iris.xlsx', sheet_name='virginica')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extensible Markup Language (XML)\n",
    "\n",
    "Reading xml files is a bit more complicated as they are much more flexible than other structured file types. XML files follow a\n",
    "tree-based structure, and use tags to define branches in the tree, indicated by angle brackets `<tag-name>` and closed with\n",
    "`</tag-name>`. The tags themselves can hold extra information known as attributes, which are indicated by `attribute=\"value\"` (as whole\n",
    "that would be `<tag-name attribute=\"value\"></tag-name>`). Also, inside those tags are it's 'children' which can be other tags or values.\n",
    "The following is an example of what an xml file looks like:\n",
    "\n",
    "```xml\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<people>\n",
    "\t<person id=\"1\">\n",
    "\t\t<name>John</name>\n",
    "\t\t<age>20</age>\n",
    "\t</person>\n",
    "\t<person id=\"2\">\n",
    "\t\t<name>Jane</name>\n",
    "\t\t<age>21</age>\n",
    "\t</person>\n",
    "</people>\n",
    "```\n",
    "\n",
    "The above XML represents the following tree structure (diagram does not show the specific values):\n",
    "\n",
    "![A tree representation of our xml example](xml.png)\n",
    "\n",
    "The tree structure makes xml files a bit more difficult to read in python. For now we will briefly look at doing so using\n",
    "the python's built-in xml library, but we will also look into reading xml/html in more detail in the future tutorials on\n",
    "web scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "tree = ET.parse('titanic.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "data = []\n",
    "for row in root:  \n",
    "    # There are rows for each passenger\n",
    "    record = {}\n",
    "    for item in row:  \n",
    "        # Each passenger only has data here\n",
    "        # Take the labelled data and keep both the tag and the value\n",
    "        record[item.tag] = item.text  \n",
    "    data.append(record)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. Create a dataframe from the file 'brainsize.csv' and print the first 5 rows.\n",
    "2. Load the file '04cars.xlsx' and print the retail price of the Mini Cooper.\n",
    "3. Load the file 'iris.xlsx' and print the mean petal length of the Setosa species.\n",
    "4. Load the file 'heart.json' and print the median age of the patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Data\n",
    "\n",
    "Once we have read the data, saving it is a simple task. It is just the reverse of reading, and the same libraries are used.\n",
    "We need to first structure the data in python into a suitable collection/structure for the writing process, then run the\n",
    "respective libraries write/save function. The following code shows examples of writing each of the discussed file types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Saving a txt file\n",
    "with open('example.txt', 'w') as f:\n",
    "    f.write('Hello World!')\n",
    "\n",
    "# Using numpy to save a csv file\n",
    "data = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "np.savetxt('example.csv', data, delimiter=',')\n",
    "\n",
    "# Using pandas to save an excel file\n",
    "data = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})\n",
    "pd.to_excel('example.xlsx', data)\n",
    "\n",
    "# Using pandas to save a json file\n",
    "pd.to_json('example.json', data)\n",
    "\n",
    "# Saving an xml file with pandas\n",
    "pd.to_xml('example.xml', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. Read `heart.json` and save it into a csv file\n",
    "2. Read `brainsize.csv` and save it into a json file\n",
    "3. Take the data from `04cars.xlsx`, `brainsize.csv`, `heart.json` and save them into sheets of single excel file called `data.xlsx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary Material: What If We Are Looking at a Different File Type?\n",
    "\n",
    "Fortunately, due to the existence of [PyPI](https://pypi.org/) and pip we can often find and install a library for reading and\n",
    "writing almost any file type. However, in there may be cases where you are dealing with a custom file type, for these you will\n",
    "need to write your own code to intelligently read the characters in the file and accordingly construct a collection of correct\n",
    "typing to make it useful. For example, if we are reading the following custom stat1100 file, `example.stat1100`:\n",
    "\n",
    "```txt\n",
    "data: object\n",
    "    1; name: str=John; age: int=20\n",
    "    2; name: str=Jane; age: int=21\n",
    "```\n",
    "\n",
    "we could write the following code, which is a modified version of the txt file reader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('example.stat1100', 'r') as f:\n",
    "    for line in f:\n",
    "        if line.startswith('data:'):\n",
    "            # Skip this line, it does not tell us anything\n",
    "            continue  \n",
    "        else:\n",
    "            # Current line value created as a dictionary\n",
    "            line_object = dict()  \n",
    "            \n",
    "            # Split the line at the ';' into a list of values\n",
    "            line_split = line.split(';')  \n",
    "            \n",
    "            # id is always the first value\n",
    "            line_object['id'] = line_split[0]  \n",
    "            \n",
    "            # Iterate through the rest of the values\n",
    "            for value in line_split[1:]:  \n",
    "                # Split the value at the ':' into a key and value\n",
    "                key, val = value.split(':')  \n",
    "            \n",
    "                # Split the value at the '=' into a type and value\n",
    "                val_type, val_data = val.split('=')  \n",
    "                \n",
    "                # Convert value and add\n",
    "                line_object[key] = getattr(__builtins__, val_type)(val_data)  \n",
    "            data.append(line_object)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we iterate through the lines in the file, and look at it's contents, we then accordingly convert them\n",
    "to a value within a dictionary that is stored in our data list. We use a new function here, `getattr` which gets a function,\n",
    "module, or variable specified by name as a string in the second argument from the module specified in the first argument. In\n",
    "this case the take the file specified typing from the builtins module, and use that to convert the value to the correct type."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
